import cv2
import face_recognition
from tensorflow.keras.models import load_model
import numpy as np

emotions = ['Anger',
            'Disgust',
            'Fear',
            'Happy',
            'Neutral',
            'Sad',
            'Surprise']

prediction = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

model_path = './model.hdf5'
model = load_model(model_path, compile=False)
model.summary()

video_capture = cv2.VideoCapture(0)
shape = 96, 96
frames = 20
face_locations = []
i = 0
np_vid = np.empty(shape=(1, frames, shape[0], shape[1], 3), dtype=np.dtype('uint8'))
np_vid[0] = 1
label = 'checking'

while True:
    # for each frame
    ret, frame = video_capture.read()

    if str(type(frame)) == "<class 'NoneType'>":
        break

    # Convert the image from BGR to RGB
    rgb_frame = frame[:, :, ::-1]

    cv2.putText(frame, "Angry: " + str(round(prediction[0], 3)), (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)
    cv2.putText(frame, "Disgust: " + str(round(prediction[1], 3)), (10, 105), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)
    cv2.putText(frame, "Fear: " + str(round(prediction[2], 3)), (10, 130), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)
    cv2.putText(frame, "Happy: " + str(round(prediction[3], 3)), (10, 155), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)
    cv2.putText(frame, "Neutral: " + str(round(prediction[4], 3)), (10, 180), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)
    cv2.putText(frame, "Sad: " + str(round(prediction[5], 3)), (10, 205), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)
    cv2.putText(frame, "Surprise: " + str(round(prediction[6], 3)), (10, 230), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)

    face_locations = face_recognition.face_locations(rgb_frame)

    if not face_locations:
        i = 0

    for top, right, bottom, left in face_locations:
        i += 1
        crop_img = frame[top:bottom, left:right]
        resized = cv2.resize(crop_img, shape)

        np_vid[0][i % frames - 1] = resized

        # draw result
        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)
        cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)

        if i % frames == 0:
            y = model(np_vid, training=False)
            index = np.unravel_index(np.argmax(y, axis=None), y.shape)
            label = emotions[index[1]]
            prediction = y[0].numpy()

    cv2.imshow('Video', frame)

    # Hit 'q' on the keyboard to quit
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

video_capture.release()
cv2.destroyAllWindows()
