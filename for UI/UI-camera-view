import cv2
import face_recognition
import requests
from tensorflow.keras.models import load_model
import numpy as np
print("v=%s" % cv2.__version__)

emotions = ['Anger',
            'Disgust',
            'Fear',
            'Happy',
            'Neutral',
            'Sad',
            'Surprise']

model_path = './model.hdf5'
model = load_model(model_path, compile=False)
print("Loaded generator...")
model.summary()

video_capture = cv2.VideoCapture(0)
shape = 96, 96
face_locations = []
i = 0
np_vid = np.empty(shape=(1, 20, 96, 96, 3), dtype=np.dtype('uint8'))
np_vid[0] = 1
label = 'checking'

while True:
    # for each frame
    ret, frame = video_capture.read()

    if str(type(frame)) == "<class 'NoneType'>":
        break

    # Convert the image from BGR to RGB
    rgb_frame = frame[:, :, ::-1]

    face_locations = face_recognition.face_locations(rgb_frame)

    for top, right, bottom, left in face_locations:
        i += 1
        crop_img = frame[top:bottom, left:right]
        resized = cv2.resize(crop_img, shape)
        # send resized to net
        # data = {'frame': resized}
        # r = requests.post('http://localhost:9001/senset')
        print(i % 20 - 1)
        np_vid[0][i % 20 - 1] = resized

        # draw result
        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)
        cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)

        if i % 20 == 0:
            y = model(np_vid, training=False)
            index = np.unravel_index(np.argmax(y, axis=None), y.shape)
            label = emotions[index[1]]

    cv2.imshow('Video', frame)

    # Hit 'q' on the keyboard to quit
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

video_capture.release()
cv2.destroyAllWindows()
